# Lecture 3: Linear Classifier
## CSC 296S - Deep Learning | Dr. Victor Chen

---

## IMPORTANT
This is the foundational lecture that bridges classical ML to deep learning. Everything here - linear classifiers, softmax, cross-entropy, and stacking layers - is exactly what neural networks are built from.

**Key insight from the slides:**

```
Neural Network = Stack of Linear Classifiers + Non-linear Activations
```

---

## 1. Linear Classifier = The Building Block

In deep learning, a linear classifier is also called:
- **Feed-forward layer**
- **Fully Connected Layer (FC)**
- **Dense Layer**

It's just a matrix multiplication between input `x` and weights `W`.

**The Formula:**

```
f(x,W) = Wx + b
```

Where:
- `x` = input vector (your data)
- `W` = weight matrix (learned parameters)
- `b` = bias vector (offset term)
- `f(x,W)` = output scores fro each class

---

# 2. Matrix Multiplication Deep Dive
### 2.1 The Dimensions

For image classification with CIFAR-10 style images:

```
Input image: 32x32x3 = 3,072 pixels (flattened to column vector)
Output: 10 class scores

Dimensions:
[10Ã—1] = [10Ã—3072] Ã— [3072Ã—1] + [10Ã—1]
s      =     W     Ã—    x     +   b
```

- `W` is a 10Ã—3072 matrix (30,720 learnable weights!)
- Each **row** of W is a "template" for one class
- `b` is a 10Ã—1 bias vector (one bias per class)

### 2.2 Simplified Example

**Setup:** 4-pixel image, 3 classes (cat/dog/ship)

```
Image (2Ã—2):         Flattened:
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”          [56]
â”‚ 56 â”‚231 â”‚    â†’     [231]
â”‚ 24 â”‚ 2  â”‚          [24]
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜          [2]
```

**The Computation:**

```
W                             x        b       s (scores)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0.2 â”‚-0.5 â”‚ 0.1 â”‚ 2.0 â”‚   â”‚ 56 â”‚   â”‚1.1 â”‚   â”‚ -96.8 â”‚ â† cat
â”‚ 1.5 â”‚ 1.3 â”‚ 2.1 â”‚ 0.0 â”‚ Ã— â”‚231 â”‚ + â”‚3.2 â”‚ = â”‚ 437.9 â”‚ â† dog
â”‚ 0   â”‚0.25 â”‚ 0.2 â”‚-0.3 â”‚   â”‚ 24 â”‚   â”‚-1.2â”‚   â”‚ 61.95 â”‚ â† ship
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜   â”‚ 2  â”‚   â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜
                            â””â”€â”€â”€â”€â”˜
```

**Prediction:** Dog (highest score = 437.9)

---

## 3. The Problem: How Do We Know if W is Good?

Given some weights W, we get scores. But how do we measure if these scores are correct?

**Example:** 3 training images, 3 classes

| Image | True Label | Cat Score | Car Score | Frog Score |
| ----- | ---------- | --------- | --------- | ---------- |
| ğŸ±    | cat        | 3.2       | 5.1       | -1.7       |
| ğŸš—    | car        | 1.3       | 4.9       | 2.0        |
| ğŸ¸    | frog       | 2.2       | 2.5       | -3.1       |

Problem: For the cat image, the car score (5.1) is higher than the cat score (3.2). This W is bad!

**We need:**
1. A **loss function** to quantify how wrong our predictions are
2. An **optimization method** to find better W (gradient descent)

---

## 4. From Scores to Probabilities: Softmax

Raw scores can be any number (positive, negative, huge, tiny). We want **probabilities** that:
- Are between 0 and 1
- Sum to 1 across all classes

### 4.1 The Softmax Function

```
P(class_i) = e^(score_i) / Î£ e^(score_j)
```

**Two operations:**
1. **Exponentiate** - Makes everything positive, amplifies differences
2. **Normalize** - divide by sum so everything adds to 1

### 4.2 Worked Example

```
Raw scores:           exp():             Normalized (softmax):
cat: 3.2      â†’    e^3.2 = 24.5    â†’      24.5/188.68 = 0.13
car: 5.1      â†’    e^5.1 = 164.0   â†’      164.0/188.68 = 0.87
frog: -1.7    â†’    e^-1.7 = 0.18   â†’      0.18/188.68 = 0.0
                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                   Sum = 188.68           Sum = 1.00
```

**Interpretation:** Model thinks there's an 87% chance it's a car, 13% chance it's a cat.

### 4.3 Terminology

| Term                      | Meaning                                            |
| ------------------------- | -------------------------------------------------- |
| **Logits**                | Raw scores before softmax (can be any real number) |
| **Softmax Probabilities** | Output after softmax (0-1, sum to 1)               |
| **Log-probabilities**     | Another name for logits (because softmax uses exp) |

---

## 5. Cross-Entropy Loss

Now we can compare predicted probabilities to the true label.

### 5.1 The True Label as One-Hot

If the true class is "cat" (class 0 out of 3):

```
True distribution : [1.0, 0.0, 0.0]
					 cat  car  frog
```

### 5.2 Cross-Entropy Formula

```
Loss = -Î£ y_true Ã— log(y_pred)
```

Since y_true is one-hot (only one 1, rest are 0s), this simplifies to:

```
Loss = -log(P_predicted for correct class)
```

### 5.3 Worked Example (IMPORTANT)

**Predicted probabilities:** [0.13, 0.87, 0.00] (cat, car, frog)
**True Label:** cat â†’ one-hot: [1, 0, 0]

```
Loss = -(1xlog(0.13) + 0xlog(0.87) + 0xlog(0.00))
	 = -log(0.13)
	 = 2.04
```

**If prediction was perfect (P(cat) = 1.0):**
```
Loss = -log(1.0) = 0 â† Perfect!
```

**If prediction was confident but not wrong** (P(cat) = 0.01):
```
Loss = -log(0.01) = 4.6 â† Very high penalty!
```

### 5.4 Key Properties of Cross-Entropy

- **Perfect prediction** â†’ Loss = 0
- **Confident wrong prediction** â†’ Loss is HUGE (Heavily penalized)
- **Uncertain prediction** â†’ Moderate loss
- This is why neural networks learn to be confident on correct answers

### 5.5 Cross-Entropy vs KL Divergence

| Metric            | Use Case                                                           |
| ----------------- | ------------------------------------------------------------------ |
| **Cross-Entropy** | Supervised learning - comparing predictions to ground truth labels |
| **KL Divergence** | Unsupervised Learning - comparing two probability distributions    |

For classification with one-hot labels, they're mathematically related, but cross-entropy is the standard loss function.

---

## 6. The Training Loop (Preview)

```
1. Forward pass: scores = Wx + b
2. Softmax: Probabilities = softmax(scores)
3. Loss: L = cross_entropy(probabilities, true_labels)
4. Backward pass: compute gradients âˆ‚L/âˆ‚W and âˆ‚L/âˆ‚b
5. Update: W = W - learning_rate x âˆ‚L/âˆ‚W
6. Repeat until loss is minimized
```

This is **gradient descent**

---

## 7. The Limitation: What if Data Isn't Linearly Separable?

### 7.1 the Problem 

Some data can't be separated by a straight line/hyperplane:
![diagram](DL-IMAGES/Non-LinearDataExample.png)

No single line can separate red from blue!

### 7.2 The Solution: Non-Linear Transform

**Idea:** Transform the data into a new space where it IS linearly separable.

Example: Cartesian (x, y) â†’ Polar (r, Î¸)

``` 
Original space (x, y):                  Transformed space (r, Î¸): 
Can't separate linearly       â†’         CAN separate linearly! 
```

![diagram](DL-IMAGES/Non-LinearTransform+LinearClassifier.png)

**Key Insight:** Non-linear functions "bend" the space to make classification easier.

---

## 8. Activation Functions (Non-Linear Transforms)

These are the non-linear functions we insert between linear layers:

| Function       | Formula                         | Range     | Notes                                 |
| -------------- | ------------------------------- | --------- | ------------------------------------- |
| **ReLU**       | `max(0, x)`                     | `[0, âˆ)`  | Most common, simple, fast             |
| **Leaky ReLU** | `max(0.01x, x)`                 | `(-âˆ, âˆ)` | Fixes "dying ReLU" problem            |
| **Sigmoid**    | `1/(1 + e^(-x))`                | `(0, 1)`  | Old-school, used in output for binary |
| **Tanh**       | `(e^x - e^(-x))/(e^x + e^(-x))` | `(-1, 1)` | Zero-centered sigmoid                 |
| **ELU**        | `x if x>0, else Î±(e^x - 1)`     | `(-Î±, âˆ)` | Smooth, can go negative               |
**ReLU is the default choice for hidden layers in modern networks.**

---

## 9. Stacking Linear Classifiers = Neural Network

### 9.1 The Key Insight

**One Linear Classifier:**
```
s = Wx
```

**Two-layer neural network:**
```
h = activation(Wâ‚x) â† Hidden layer
s = Wâ‚‚h â† Output layer 
```

**Or written together:**
```
s = Wâ‚‚ Ã— activation(Wâ‚ Ã— x)
```

### 9.2 Dimensions Example

``` 
Input: x âˆˆ â„Â³â°â·Â² (32Ã—32Ã—3 image flattened) 
Hidden: h âˆˆ â„Â¹â°â° (100 hidden units) 
Output: s âˆˆ â„Â¹â° (10 class scores) 
Wâ‚: 100Ã—3072 (maps input to hidden) 
Wâ‚‚: 10Ã—100 (maps hidden to output) 
```

``` 
x â”€â”€[Wâ‚]â”€â”€â†’ h â”€â”€[Wâ‚‚]â”€â”€â†’ s 
3072       100          10 
```

### 9.3 Why Does This Work?

- **Wâ‚** learns to extract useful features from raw pixels
- **Activation** introduces non-linearity (bends the space)
- **Wâ‚‚** learns to classify based on those features

Without the activation function, stacking linear layers is useless:

```
Wâ‚‚(Wâ‚x) = (Wâ‚‚Wâ‚)x = W'x â† Still just a linear function!
```

The activation is essential - it's what gives neural networks their power.

### 9.4 Going Deeper

**3-Layer Network:**
```
hâ‚ = activation(Wâ‚x) 
hâ‚‚ = activation(Wâ‚‚hâ‚) 
s = Wâ‚ƒhâ‚‚
```

**The Pattern:**
```
Linear â†’ Activation â†’ Linear â†’ Activation â†’ ... â†’ Linear â†’ Softmax â†’ Cross entropy
```

More layers = more capacity to learn complex patterns. This is where "deep" learning comes from.

---

## 10. Graphical View: Fully Connected Networks

![diagram](DL-IMAGES/FCNN.png)

- Every Input connects to every hidden unit (fully connected)
- Every hidden unit connects to every output
- Each connection has a weight (learned parameter)

**How many linear classifiers are there in a network with 2 hidden layers?**
There are 3 linear classifiers (Inputâ†’hidden1, hidden1â†’hidden2, hidden2â†’output)

---

## 11. PyTorch Implementation

### 11.1 Single Linear Layer

```python
import torch
import torch.nn as nn

# Linear layer: 20 inputs -> 30 outputs
m = nn.linear(20,30)

# Batch of 128 samples, each with 20 features
input = torch.randn(128, 20)
output = m(input)

print(output.size()) # torch.Size([128, 30])
print(m.weight.size()) # torch.Size([30, 20])
print(m.bias.size()) # torch.Size([30])
```

### 11.2 Linear Classifier (with Softmax)

```python
import torch.nn as nn
import torch.nn.functional as F

class LinearClassifier(nn.Module):
	def __init__(self):
		super().__init__()
		self.linear = nn.Linear(256, 10) # 256 features -> 10 classes
		
	def forward(self, x):
		x = self.linear(x)       # Raw Scores (logits)
		x = F.softmax(x, dim=1)  # Converts to probabilities
		return x
```

### 11.3 Two-Layer Neural Network

```python
class TwoLayerNet(nn.Module):
	def __init__(self):
		super().__init__()
		self.fc1 = nn.Linear(3072, 100) # Input -> Hidden
		self.fc2 = nn.Linear(100, 10) # Hidden -> Output
		
	def forward(self, x):
		x = self.fc1(x) # Linear Transform
		x = F.relu(x) # Non-Linearity
		x = self.fc2(x) # Linear Transform
		x = F.softmax(x, dim=1) # Probabilities
		return x
```

---

## 12. Key Takeaways from Lecture 3

1. **Linear Classifier: f(x, W) = Wx + b** - the fundamental building block
2. **Softmax converts scores to probabilities** - exp() then normalize
3. **Cross-entropy measures prediction errors** - heavily penalizes confident wrong answers
4. **Linear Classifiers can't handle Non-Linear Data** - they can only draw straight decision boundaries
5. **Activation functions introduce non-linearity** - ReLU is the modern default
6. **Neural networks = stacked linear classifiers + activations** - this is literally what deep learning is
7. **Without activations, stacking is pointless** - Wâ‚‚(Wâ‚x) = W'x, still linear
8. **More layers = more capacity** - can learn increasingly complex patterns
9. **PyTorch makes this easy** - nn.Linear + F.relu + F.softmax
10. **Next up: How do we actually find good W?** - Gradient descent and back-propagation.

---

## Quick Reference : The Full Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FORWARD PASS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Image â†’ Flatten â†’ Linear(Wâ‚) â†’ ReLU â†’ Linear(Wâ‚‚) â†’ Softmax   â”‚
â”‚ [32Ã—32Ã—3]  [3072]    [100]        [100]  [10]         [10]      â”‚
â”‚                                                    probabilitiesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
								â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              LOSS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Cross-Entropy(predicted_probs, true_label) â†’ scalar loss    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
								â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKWARD PASS (NEXT LECTURE)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Compute gradients â†’ Update Wâ‚, Wâ‚‚ â†’ Repeat           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


---

*Completed*